x-airflow-image: &airflow_image starias/airflow-etl
x-spark-common-properties: &spark_common_properties
  image: bitnami/spark:3.5.3
  volumes:
   - .jobs:/opt/bitnami/spark/jobs
  networks:
    - backend
  restart: always
  
x-spark-worker-environment: &spark_worker_environment
  SPARK_MODE: worker
  SPARK_WORKER_CORES: 2
  SPARK_WORKER_MEMORY: 8G
  SPARK_MASTER_URL: spark://spark-master:7077
  TRAINED_MODELS_PATH: ${TRAINED_MODELS_PATH}

x-environment: &airflow_environment
  - AIRFLOW__CORE__EXECUTOR=CeleryExecutor
  - AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://airflow:airflow@postgres/airflow
  - AIRFLOW__CELERY__BROKER_URL=redis://:@redis:6379/2
  - AIRFLOW__CORE__FERNET_KEY=''
  #- AIRFLOW__CORE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__DETABASE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__STORE_DAG_CODE=True
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True
  - AIRFLOW_CONN_POSTGRES_DEFAULT=postgresql://airflow:airflow@postgres/airflow
  - AIRFLOW__CORE__FERNET_KEY=EJ7tt4r06HhjniZKjNICTqaOGotA2Jfa9EF7KyW-7Wc=
  - AIRFLOW__CELERY__WORKERS=2
  - AIRFLOW__CELERY__WORKER_CONCURRENCY=2
  #- AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:B9#fS!4qW@p8zK*1@postgres/airflow-db
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
  - CHECKPOINT_DIR=${CHECKPOINT_DIR}
  
  #- MONGO_DB_URI=${MONGO_DB_URI}
  - AIRFLOW__CORE__DEFAULT_TIMEZONE=UTC
  - AIRFLOW__EMAIL__EMAIL_BACKEND=airflow.utils.email.send_email_smtp
  - AIRFLOW__SMTP__SMTP_HOST=smtp.gmail.com
  - AIRFLOW__SMTP__START_TLS=False
  - AIRFLOW__SMTP__SMTP_SSL=False
  - AIRFLOW__SMTP__SMTP_PORT=587
  - TRAINED_MODELS_PATH=${TRAINED_MODELS_PATH}
  - REDIS_HOST=${REDIS_HOST}

  - NEWS_COUNTRIES_PATH=${NEWS_COUNTRIES_PATH}
  - ETL_LOGS_PATH=${ETL_LOGS_PATH}
  - STAGING_AREA_PATH=${STAGING_AREA_PATH}
  
services:
  postgres:
    restart: always
    image: postgres:14
    container_name: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      #POSTGRES_DB: admin
    networks:
      - backend
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - /home/starias/africa_news_api/database/init:/docker-entrypoint-initdb.d
      

  redis:
    image: redis:latest
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - backend
    volumes:
      - ./data/redis:/data
    restart: always

  airflow-init:
    container_name: airflow-init
    image: *airflow_image
    depends_on:
      - postgres
    environment: *airflow_environment
    secrets:
    - airflow_user
    - airflow_email
    - airflow_password
    - airflow_firstname
    - airflow_lastname
    volumes:
      - ./scripts/airflow-scripts/airflow-init-entrypoint.sh:/entrypoint.sh
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    networks:
      - backend
    
  
  airflow-flower:
        image: *airflow_image
        container_name: airflow-flower
        restart: always
        depends_on:
            - redis
        environment: *airflow_environment
        ports:
            - "5555:5555"
        command: celery flower
        networks:
          - backend
  


  airflow-webserver:
    restart: always
    container_name: airflow-webserver
    image: *airflow_image
    depends_on:
      - airflow-init
    environment: *airflow_environment
    ports:
      - "8080:8080"
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      #- ./trained_models:/opt/trained_models/
      - ./data/logs/airflow_logs:/opt/airflow/logs
      - ./data/logs/etl_logs:/opt/etl_logs
      - ./data/staging_area:/opt/staging_area
      - ./news_countries:/opt/news_countries
      #- ./config:/opt/config
      - ./src:/opt/src
      - ./scripts/airflow-scripts/entrypoint.sh:/entrypoint.sh
    command: webserver
    secrets:
      - smtp_user
      - smtp_password
      - smtp_mail_from
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    networks:
      - backend
  airflow-scheduler:
    restart: always
    container_name: airflow-scheduler 
    image: *airflow_image
    depends_on: 
      - airflow-init
    environment: *airflow_environment
    volumes:
      - ./airflow_dags:/opt/airflow/dags
      - ./data/logs/airflow_logs:/opt/airflow/logs
      - ./data/logs/etl_logs:/opt/etl_logs
      - ./data/staging_area:/opt/staging_area
      - ./news_countries:/opt/news_countries
      #- ./config:/opt/config
      - ./src:/opt/src
      #- ./trained_models:/opt/trained_models/
      - ./scripts/airflow-scripts/entrypoint.sh:/entrypoint.sh   
    command: scheduler
    networks:
      - backend
    secrets:
      - smtp_user
      - smtp_password
      - smtp_mail_from
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
    
  airflow-worker:
    container_name: airflow-worker
    image: *airflow_image
    depends_on:
      - airflow-scheduler
    environment: *airflow_environment
    networks:
      - backend
    restart: always
    volumes:
      - ./airflow_dags:/opt/airflow/dags/
      - ./data/logs/airflow_logs:/opt/airflow/logs
      - ./data/logs/etl_logs:/opt/etl_logs
      - ./data/staging_area:/opt/staging_area
      - ./news_countries:/opt/news_countries
      #- ./config/:/opt/config/
      - ./src/:/opt/src/
      #- ./scripts:/opt/scripts/
      #- ./trained_models/:/opt/trained_models/
      #- ./data/checkpoint/filtered_news/:/opt/airflow/checkpoint/filtered_news/
      #- ./data/checkpoint/processed_news/:/opt/airflow/checkpoint/processed_news/
      #- ./data/checkpoint/available_news/:/opt/airflow/checkpoint/available_news/
      - ./scripts/airflow-scripts/entrypoint.sh:/entrypoint.sh
    command: celery worker
    secrets:
      - smtp_user
      - smtp_password
      - smtp_mail_from
    entrypoint: ["/bin/bash", "/entrypoint.sh"]
  
  spark-master:
    <<: *spark_common_properties
    container_name: spark-master
    restart: always
    ports:
      - "9090:8080"
      - "7077:7077"
    #command: bin/spark-class org.apache.spark.deploy.master.Master
    command: >
      /bin/bash -c "
      pip install unidecode  &&
      bin/spark-class org.apache.spark.deploy.master.Master
      "
    volumes:
    - ./data/staging_area:/opt/staging_area
      #- ./trained_models:/opt/trained_models/
      #- ./data/checkpoint/filtered_news/:/opt/airflow/checkpoint/filtered_news/
      #- ./data/checkpoint/processed_news/:/opt/airflow/checkpoint/processed_news/
      #- ./data/checkpoint/available_news/:/opt/airflow/checkpoint/available_news/
    networks:
      - backend
    environment:
      - TRAINED_MODELS_PATH=${TRAINED_MODELS_PATH}
      
      #- SPARK_MODE=master
      #- SPARK_RPC_AUTHENTICATION_ENABLED=no
      #- SPARK_RPC_ENCRYPTION_ENABLED=no

  spark-worker1:
    <<: *spark_common_properties
    container_name: spark-worker1
    #command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    command: >
      /bin/bash -c "
      pip install unidecode && 
      bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
      "
    depends_on:
      - spark-master
    environment: *spark_worker_environment
    volumes:
      - ./data/logs/etl_logs:/opt/etl_logs
      - ./data/staging_area:/opt/staging_area
      #- ./trained_models:/opt/trained_models
      #- ./nltk_data:/opt/nltk_data/
      #- ./data/checkpoint/filtered_news/:/opt/airflow/checkpoint/filtered_news/
      #- ./data/checkpoint/processed_news/:/opt/airflow/checkpoint/processed_news/
      #- ./data/checkpoint/available_news/:/opt/airflow/checkpoint/available_news/

networks:
  backend:
    name: backend-network 
    driver: bridge
  

#volumes:
  #postgres_data:

secrets:
  smtp_user:
    file: ./secrets/smtp_user/user.txt
  smtp_password:
    file: ./secrets/smtp_user/password.txt
  smtp_mail_from:
    file: ./secrets/smtp_user/mail_from.txt
  airflow_user:
    file: ./secrets/airflow_user/username.txt
  airflow_email:
    file: ./secrets/airflow_user/email.txt
  airflow_password:
    file: ./secrets/airflow_user/password.txt
  airflow_firstname:
    file: ./secrets/airflow_user/firstname.txt
  airflow_lastname:
    file: ./secrets/airflow_user/lastname.txt
    